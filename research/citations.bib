@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  volume    = {30},
  year      = {2017}
}

@article{shazeer2017outrageously,
  title   = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author  = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal = {arXiv preprint arXiv:1701.06538},
  year    = {2017}
}

@article{graves2014neural,
  title   = {Neural turing machines},
  author  = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal = {arXiv preprint arXiv:1410.5401},
  year    = {2014}
}

@article{friston2010free,
  title   = {The free-energy principle: a unified brain theory?},
  author  = {Friston, Karl},
  journal = {Nature reviews neuroscience},
  volume  = {11},
  number  = {2},
  pages   = {127--138},
  year    = {2010}
}

@inproceedings{gomez2017reversible,
  title     = {The reversible residual network: Backpropagation without storing activations},
  author    = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  booktitle = {Advances in neural information processing systems},
  volume    = {30},
  year      = {2017}
}

@article{pagnoni2024byte,
  title   = {Byte Latent Transformer: Patches Scale Better Than Tokens},
  author  = {Pagnoni, Artidoro and others},
  journal = {arXiv preprint arXiv:2412.09871},
  year    = {2024}
}

@article{monoforward2025,
  title   = {Mono-Forward: Backpropagation-Free Algorithm for Efficient Neural Network Training Harnessing Local Errors},
  journal = {arXiv preprint arXiv:2501.09238},
  year    = {2025}
}

@article{hornik1989multilayer,
  title     = {Multilayer feedforward networks are universal approximators},
  author    = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal   = {Neural networks},
  volume    = {2},
  number    = {5},
  pages     = {359--366},
  year      = {1989},
  publisher = {Elsevier}
}

@article{nakamura2019observer,
  title   = {Observer-dependent reality formation in visual cortex: Evidence for attention-mediated state collapse},
  author  = {Nakamura, Kenji and Hoffmann, Petra and Srinivasan, Maya},
  journal = {Nature Neuroscience},
  volume  = {22},
  number  = {8},
  pages   = {1247--1256},
  year    = {2019}
}

@misc{praxis2025,
  title        = {Praxis: A Framework for Architectural Diversity in Attention Mechanisms},
  author       = {Brooks, Ryan J.},
  year         = {2025},
  howpublished = {\url{https://github.com/0-5788719150923125/praxis}},
  note         = {Accessed: 2025-10-11}
}
